{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction to TensorFlow\n",
    "**KIAS TensorFlow Tutorial**  \n",
    "**Dec 19, 2016. Sangwoong Yoon, [IDNSTORY INC.](http://haezoom.com/)**  \n",
    "\n",
    "** Table of Contents **  \n",
    "1. TensorFlow - What and Why  \n",
    "    1. High-Level Deep Learning Libraries  \n",
    "    * Automatic Differentiation  \n",
    "    * Flexible CPU/GPU deployment  \n",
    "2. TensorFlow Basic Mechanics  \n",
    "    2. Graph + Session  \n",
    "    2. Tensor + Flow  \n",
    "3. Example: Optimization  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TensorFlow - What and Why\n",
    "* [TensorFlow](https://www.tensorflow.org/) is a **Google-powered** open-source scientific computing library.\n",
    "![TensorFlow Logo](https://avatars0.githubusercontent.com/u/15658638?v=3&s=400)\n",
    "* TensorFlow is the latest one of many **high-level deep learning libraries**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. High-Level Deep Learning Libraries, or \"Deep Learning Frameworks\"\n",
    "* Implementing and experimenting with complex deep neural networks are **painful**.  \n",
    "    1. Everytime you **modify the model structure**, you need to adjust the training code accordingly.\n",
    "        * Backpropagation algorithm, or gradient descent, algorithm depends on the network structure.\n",
    "    2. For large networks, **GPU computing** is almost necessary.   \n",
    "        * The sizes of both model and data are very large.\n",
    "    \n",
    "![DNN](http://neuralnetworksanddeeplearning.com/images/tikz41.png)\n",
    "(the image from http://neuralnetworksanddeeplearning.com/)\n",
    "    \n",
    "    \n",
    "* There have been **high-level deep learning libraries** which try to alleviate the pain.\n",
    "    * [Caffe](http://caffe.berkeleyvision.org/)\n",
    "        * UC Berkeley. C++. Computer vision oriented.\n",
    "    * [Theano](http://deeplearning.net/software/theano/)\n",
    "        * U of Montreal. Python. Very general.\n",
    "    * [Torch](http://torch.ch/)\n",
    "        * Selected by Facebook AI Research. LuaJIT. Very general.\n",
    "    * [MXNet](http://mxnet.io/)\n",
    "        * Selected by Amazon. Supports multiple languages and environments.\n",
    "    * [TensorFlow](https://www.tensorflow.org/)\n",
    "        * Google. C++ and Python. Intuitive and easy to use. Most popular. Rapidly developing.\n",
    "* In the essence, the above libraries all have similar features: **auto-diff**, and **flexible deployment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Automatic Differentiation\n",
    "![autodiff](http://elekslabs.com/wp-content/uploads/2013/07/calculus.jpg)\n",
    "(image from http://elekslabs.com/2013/07/a-short-note-on-automatic.html)\n",
    "\n",
    "* **Key Idea**\n",
    "    * Training of a neural network, the error backpropagation, is simply differentiation\n",
    "    * Differentiation follows simple rules\n",
    "        * for example, $(f(g(x)))' = f'(g(x))g'(x)$\n",
    "    * It can be **automated** !\n",
    "* It works like a magic.\n",
    "    * You specify a graph structure\n",
    "    * You pick which variables to be differentiated with respect to which variables.\n",
    "    * Boom! Gradient available!\n",
    "    \n",
    "** Now, you can experiment with network structures more rapidly **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Flexible Deployment : One code, for CPU and GPU\n",
    "![nvidia](http://cms.ipressroom.com.s3.amazonaws.com/219/files/20149/543c131bfe058b228e020181_slide1/slide1_mid.jpg)\n",
    "* The era of GPGPU (General Purpose Graphic Processing Unit)\n",
    "    * NVIDIA provides CUDA to code with GPU's.\n",
    "    * However, the abstraction level of CUDA is too low for machine learning research and development.\n",
    "    \n",
    "    \n",
    "    \n",
    "* The deep learning frameworks do the dirty things for you.\n",
    "    * The libraries compile your high-level codes (in Python or Lua) into low-level binaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TensorFlow Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. TensorFlow = Graph + Session\n",
    "** Typical TensorFlow workflow**\n",
    "> 1. Define a **graph**  \n",
    "> 2. Lauch the defined graph through **session**\n",
    "\n",
    "\n",
    "* Graph: The relationship between the input and the output\n",
    "* Session: The environment that the graph is executed\n",
    "\n",
    "Session takes a node as an input, and traces back the graph to collect information needed to evaluate the node.\n",
    "\n",
    "\n",
    "To make analogy with programming languages, \n",
    "* **graph** is defining a function\n",
    "* Launching a **session** is run the function with the input arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Define an empty graph'''\n",
    "g = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Define a node in the graph'''\n",
    "with g.as_default():  \n",
    "    hi = tf.constant(5.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Session example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "with g.as_default():  \n",
    "    '''Open a session'''\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    '''Launch (or execute) the graph through session'''\n",
    "    print sess.run(hi)\n",
    "    \n",
    "'''close session'''\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Session example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "with g.as_default():  \n",
    "    '''Open a session'''\n",
    "    with tf.Session() as sess:\n",
    "        '''Launch (or execute) the graph through session'''\n",
    "        print sess.run(hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A graph is only evaluated through a session\n",
    "* `with` statement is to make graph/session assignment clear.\n",
    "\n",
    "\n",
    "* Actually, there are several possible ways to launch a graph and a session\n",
    "    * matter of style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Session example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "    '''Launch (or execute) the graph through session'''\n",
    "    print sess.run(hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Graph = Tensor + Flow\n",
    "* **Tensor** : A node in a graph that has N-dimensional array as values\n",
    "* **Operation** : A node in a graph that takes a set of tensors as the input, and produce a set of tensors as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    a = tf.constant(5)\n",
    "    b = tf.constant(10)\n",
    "    c = a + b  # or c = tf.sum(a,b)\n",
    "    \n",
    "sess = tf.Session(graph=g)\n",
    "sess.run(c)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here, `c` is a **summation operator** that adds two tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensors = {Constants, Placeholders, Variables}\n",
    "* **Constants** : A tensor whose value does not change.\n",
    "* **Placeholders**: A tensor whose value is given **when a session is executed**\n",
    "    * Typically denotes input/output **data**.\n",
    "* **Variables**: A tensor whose value is maintained through multiple session runs\n",
    "    * Typically denotes **model parameters**(connection weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there are **placeholders** involved in the computation, **sess.run** needs **feed_dict** to plug values into the placeholders   \n",
    "``` feed_dict = {tensor : value} ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    a = tf.placeholder('float32')\n",
    "    b = tf.constant(10.)\n",
    "    c = a + b  # or c = tf.sum(a,b)\n",
    "    \n",
    "sess = tf.Session(graph=g)\n",
    "print sess.run(c, feed_dict={a:np.array(5.)})  # we need feed_dict\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables** maintain their states, and have to be initialized by a separate operator.  \n",
    "Typically, `tf.tf.global_variables_initializer()` are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    a = tf.placeholder('float32')\n",
    "    b = tf.Variable(initial_value=10., dtype='float32')\n",
    "    init = tf.global_variables_initializer()  # it initializes all variables\n",
    "    c = b.assign_add(a) # basically means b = b + a\n",
    "    \n",
    "sess = tf.Session(graph=g)\n",
    "sess.run(init)  # initialization should be run first\n",
    "print sess.run(c, feed_dict={a:np.array(5.)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state of **b** remains after the execution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.0\n",
      "25.0\n",
      "30.0\n",
      "35.0\n",
      "40.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print sess.run(c, feed_dict={a:np.array(5.)})\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Note that all the tensors are strictly aware of data types and shapes ! **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimization with the Automatic Differentiation\n",
    "* Conveniently, TensorFlow provides high-level functions for gradient descent optimization\n",
    "    * ```tf.train.GradientDescentOptimizer```\n",
    "    * ```tf.train.AdagradOptimizer```\n",
    "    * ```tf.train.AdamOptimizer```\n",
    "* ```tf.train.GradientDescentOptimizer```, as an operator, kindly computes gradients and update the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.0, None]\n",
      "[9.8999996, None]\n",
      "[9.802, None]\n",
      "[9.7059603, None]\n",
      "[9.6118412, None]\n",
      "[9.5196047, None]\n",
      "[9.4292126, None]\n",
      "[9.3406286, None]\n",
      "[9.2538157, None]\n",
      "[9.1687393, None]\n"
     ]
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    a = tf.placeholder('float32')\n",
    "    b = tf.Variable(initial_value=10., dtype='float32')\n",
    "    \n",
    "    init = tf.global_variables_initializer()  # it initializes all variables\n",
    "    \n",
    "    # objective function\n",
    "    loss = tf.pow(tf.sub(a,b),2)  # squared distance between two scalars\n",
    "    \n",
    "    # optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    \n",
    "sess = tf.Session(graph=g)\n",
    "sess.run(init)  # initialization should be run first\n",
    "\n",
    "for i in xrange(10):\n",
    "    print sess.run([b, train_op], feed_dict={a:np.array(5.)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The code make the variable **b** closer to the placeholder **a**\n",
    "    * We have never specified the gradient of the loss function!\n",
    "* **train_op**, operation that performs the training, does not return anything.\n",
    "* If **sess.run()** get a list as an input, it outputs a list as well.\n",
    "* Do more iteration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in xrange(10):\n",
    "    print sess.run([b, train_op], feed_dict={a:np.array(5.)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap-Up\n",
    "* TensorFlow is one of many deep learning frameworks that provide high-level functions for rapid implementation of deep neural networks.\n",
    "* TensorFlow works by\n",
    "    1. Defining a **graph**\n",
    "    2. Launching it through a **session**\n",
    "* Gradient descent is super easy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
